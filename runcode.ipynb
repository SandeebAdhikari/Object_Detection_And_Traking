{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Detection: object_detection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model and set to evaluation mode\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "video_paths = [\n",
    "    '/Users/sandeebadhikari/Documents/cs370-assignments/Project-1/YouTube-Videos/Cyclist and vehicle Tracking - 1.mp4',\n",
    "    '/Users/sandeebadhikari/Documents/cs370-assignments/Project-1/YouTube-Videos/Cyclist and vehicle tracking - 2.mp4',\n",
    "    '/Users/sandeebadhikari/Documents/cs370-assignments/Project-1/YouTube-Videos/Drone Tracking Video.mp4'\n",
    "]\n",
    "\n",
    "def detect_objects_in_frame(frame, model):\n",
    "    # Convert the color space from BGR (OpenCV) to RGB (PIL)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame_rgb)\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    output = model(image)\n",
    "    \n",
    "    # Draw detected objects with a high score\n",
    "    for idx, score in enumerate(output[0]['scores']):\n",
    "        if score > 0.1:  # Confidence threshold\n",
    "            label = output[0]['labels'][idx].item()\n",
    "            if label in [2, 3]:  # COCO IDs: 2 for bicycle, 3 for car\n",
    "                box = output[0]['boxes'][idx].tolist()\n",
    "                # Convert box coordinates to integers\n",
    "                box = list(map(int, box))\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "                label_text = f\"{'Bicycle' if label == 2 else 'Car'}: {score:.2f}\"\n",
    "                cv2.putText(frame, label_text, (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "for video_path in video_paths:\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    frame_rate = 20  # Extract frame every 30 seconds based on actual FPS\n",
    "    \n",
    "    frame_number = 0\n",
    "    \n",
    "    while True:\n",
    "        success, frame = capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        frame_number += 1\n",
    "        \n",
    "        if frame_number % frame_rate == 0:\n",
    "            frame_with_detections = detect_objects_in_frame(frame, model)\n",
    "            # Display the frame with detections\n",
    "            cv2.imshow(\"Frame\", frame_with_detections)\n",
    "            \n",
    "            # Break the loop when 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kalman_filter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model and set to evaluation mode\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "video_paths = [\n",
    "    '/Users/sandeebadhikari/Documents/cs370-assignments/Project-1/YouTube-Videos/Cyclist and vehicle Tracking - 1.mp4',\n",
    "    '/Users/sandeebadhikari/Documents/cs370-assignments/Project-1/YouTube-Videos/Cyclist and vehicle tracking - 2.mp4',\n",
    "    '/Users/sandeebadhikari/Documents/cs370-assignments/Project-1/YouTube-Videos/Drone Tracking Video.mp4'\n",
    "]\n",
    "    \n",
    "def detect_objects_in_frame(frame, model):\n",
    "    detections = []\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert the color space from BGR (OpenCV) to RGB (PIL)\n",
    "    image = Image.fromarray(frame_rgb)\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    output = model(image)\n",
    "    \n",
    "    # Draw detected objects with a high score\n",
    "    for idx, score in enumerate(output[0]['scores']):\n",
    "        \n",
    "        if score > 0.1:  # Confidence threshold\n",
    "            label = output[0]['labels'][idx].item()\n",
    "            if label in [2, 3]:  # COCO IDs: 2 for bicycle, 3 for car\n",
    "                box = output[0]['boxes'][idx].tolist()\n",
    "                # Convert box coordinates to integers\n",
    "                detection = {'box': box, 'score': score, 'label': label}\n",
    "                detections.append(detection)\n",
    "                box = list(map(int, box))\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "                label_text = f\"{'Bicycle' if label == 2 else 'Car'}: {score:.2f}\"\n",
    "                cv2.putText(frame, label_text, (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        return frame, detections\n",
    "\n",
    "\n",
    "def initialize_kalman_filter():\n",
    "    kf = KalmanFilter(dim_x=4, dim_z=2)  # 4 state variables, 2 measurements (x and y positions)\n",
    "    kf.F = np.array([[1, 0, 1, 0],  # State transition matrix\n",
    "                     [0, 1, 0, 1],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "    kf.H = np.array([[1, 0, 0, 0],  # Measurement function\n",
    "                     [0, 1, 0, 0]])\n",
    "    kf.R *= np.array([[1, 0],       # Measurement noise\n",
    "                      [0, 1]]) \n",
    "    kf.P *= 10.                     # Initial state covariance\n",
    "    kf.Q = Q_discrete_white_noise(dim=2, dt=1, var=0.01, block_size=2)  # Process noise\n",
    "    return kf\n",
    "\n",
    "kf = initialize_kalman_filter()\n",
    "\n",
    "def update_kalman_filter(kf, detection):\n",
    "    # Assume detection is a bounding box [x1, y1, x2, y2]\n",
    "    x_center = (detection[0] + detection[2]) / 2\n",
    "    y_center = (detection[1] + detection[3]) / 2\n",
    "    \n",
    "    # Update Kalman filter with detected position\n",
    "    kf.update(np.array([x_center, y_center]))\n",
    "    \n",
    "    # Predict next state\n",
    "    kf.predict()\n",
    "    \n",
    "    return kf\n",
    "\n",
    "trackers = []\n",
    "\n",
    "def match_detection_to_tracker(detection, trackers):\n",
    "    min_distance = float('inf')\n",
    "    matched_tracker_index = None\n",
    "    \n",
    "    detection_center = np.array([(detection['box'][0] + detection['box'][2]) / 2, \n",
    "                                 (detection['box'][1] + detection['box'][3]) / 2])\n",
    "    \n",
    "    for i, tracker in enumerate(trackers):\n",
    "        predicted_position = tracker['predicted_position']\n",
    "        distance = np.linalg.norm(detection_center - predicted_position)\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            matched_tracker_index = i\n",
    "    \n",
    "    distance_threshold = 50  \n",
    "    \n",
    "    if min_distance < distance_threshold:\n",
    "        return matched_tracker_index\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "for video_path in video_paths:\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    frame_rate = 20  # Extract frame every 20 seconds \n",
    "    \n",
    "    frame_number = 0\n",
    "    \n",
    "    while True:\n",
    "        success, frame = capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frame_for_prediction = frame.copy()\n",
    "        \n",
    "        frame_number += 1 \n",
    "        \n",
    "        if frame_number % frame_rate == 0:\n",
    "            frame_with_detections, detections = detect_objects_in_frame(frame, model)\n",
    "    \n",
    "            for detection in detections:\n",
    "                matched_tracker_index = match_detection_to_tracker(detection, trackers)\n",
    "                if matched_tracker_index is not None:\n",
    "                    kf = trackers[matched_tracker_index]['kf']\n",
    "                    update_kalman_filter(kf, detection['box'])\n",
    "                    # Extract the updated position after prediction and update\n",
    "                    x_position, y_position = int(kf.x[0, 0]), int(kf.x[1, 0])\n",
    "                    trackers[matched_tracker_index]['predicted_position'] = np.array([x_position, y_position])\n",
    "                else:\n",
    "                    new_kf = initialize_kalman_filter()\n",
    "                    # Assuming the detection's center as the initial position\n",
    "                    x_center = (detection['box'][0] + detection['box'][2]) / 2\n",
    "                    y_center = (detection['box'][1] + detection['box'][3]) / 2\n",
    "                    new_kf.x = np.array([x_center, y_center, 0., 0.]).reshape(4, 1)  # Corrected shape for state vector\n",
    "                    new_tracker = {\n",
    "                        'kf': new_kf,\n",
    "                        'predicted_position': np.array([x_center, y_center]),  # Initialize predicted_position\n",
    "                        'positions': [(int(x_center), int(y_center))]  # Initialize positions list with the current position\n",
    "                    }\n",
    "                    trackers.append(new_tracker)\n",
    "\n",
    "                \n",
    "            # Correct way to access the 'kf' key of the first tracker in the list, as an example\n",
    "            if trackers:  # Ensure the list is not empty\n",
    "                kf = trackers[0]['kf']  # Accessing the first tracker and then its 'kf' key\n",
    "\n",
    "            trajectory_frame = np.zeros_like(frame_for_prediction)\n",
    "            \n",
    "            for tracker in trackers:\n",
    "                kf = tracker['kf']\n",
    "                kf.predict()\n",
    "                x_position, y_position = int(kf.x[0, 0]), int(kf.x[1, 0])\n",
    "                #print(\"kf is defined:\", 'kf' in locals() or 'kf' in globals())  # Check if kf is defined\n",
    "                #print(\"Type of kf:\", type(kf))  # Check the type of kf\n",
    "    \n",
    "                if 'positions' not in tracker:\n",
    "                    tracker['positions'] = []\n",
    "                tracker['positions'].append((x_position, y_position))\n",
    "    \n",
    "                for i in range(1, len(tracker['positions'])):\n",
    "                    cv2.line(trajectory_frame, tracker['positions'][i - 1], tracker['positions'][i], (0, 255, 0), 2)\n",
    "                \n",
    "            combined_frame = cv2.addWeighted(frame, 0.8, trajectory_frame, 1, 0)\n",
    "\n",
    "            cv2.imshow(\"Trajectories\", combined_frame)     \n",
    "            #cv2.imshow(\"Frame\", frame_with_detections)\n",
    "            \n",
    "            # Break the loop when 'q' is pressed\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "             \n",
    "    capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
